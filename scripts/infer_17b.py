import os
from pathlib import Path

import cv2
import torch
import pickle
import imageio
import numpy as np
import tyro

from PIL import Image
from decord import VideoReader
from omegaconf import OmegaConf
from torchvision import transforms
from transformers import AutoTokenizer
from huggingface_hub import snapshot_download
from diffusers import FlowMatchEulerDiscreteScheduler

from mtvcrafter.models import (
    MTVCrafterPipeline17B,
    Transformer17B,
    AutoencoderKLWan,
    CLIPModel,
    WanT5EncoderModel,
    get_1d_rotary_pos_embed,
    get_3d_motion_spatial_embed,
)
from mtvcrafter.models import SMPL_VQVAE, VectorQuantizer, Encoder, Decoder
from mtvcrafter.wan_lora import WanLoraWrapper
from draw_pose import get_control_conditions


# inference with dynamic resolutions
ASPECT_RATIO_512 = {
    "0.25": [256.0, 1024.0],
    "0.26": [256.0, 992.0],
    "0.27": [256.0, 960.0],
    "0.28": [256.0, 928.0],
    "0.32": [288.0, 896.0],
    "0.33": [288.0, 864.0],
    "0.35": [288.0, 832.0],
    "0.4": [320.0, 800.0],
    "0.42": [320.0, 768.0],
    "0.48": [352.0, 736.0],
    "0.5": [352.0, 704.0],
    "0.52": [352.0, 672.0],
    "0.57": [384.0, 672.0],
    "0.6": [384.0, 640.0],
    "0.68": [416.0, 608.0],
    "0.72": [416.0, 576.0],
    "0.78": [448.0, 576.0],
    "0.82": [448.0, 544.0],
    "0.88": [480.0, 544.0],
    "0.94": [480.0, 512.0],
    "1.0": [512.0, 512.0],
    "1.07": [512.0, 480.0],
    "1.13": [544.0, 480.0],
    "1.21": [544.0, 448.0],
    "1.29": [576.0, 448.0],
    "1.38": [576.0, 416.0],
    "1.46": [608.0, 416.0],
    "1.67": [640.0, 384.0],
    "1.75": [672.0, 384.0],
    "2.0": [704.0, 352.0],
    "2.09": [736.0, 352.0],
    "2.4": [768.0, 320.0],
    "2.5": [800.0, 320.0],
    "2.89": [832.0, 288.0],
    "3.0": [864.0, 288.0],
    "3.11": [896.0, 288.0],
    "3.62": [928.0, 256.0],
    "3.75": [960.0, 256.0],
    "3.88": [992.0, 256.0],
    "4.0": [1024.0, 256.0],
}


def filter_kwargs(cls, kwargs):
    import inspect

    sig = inspect.signature(cls.__init__)
    valid_params = set(sig.parameters.keys()) - {"self", "cls"}
    filtered_kwargs = {k: v for k, v in kwargs.items() if k in valid_params}
    return filtered_kwargs


def concat_and_save(video_list, save_path, rescale_params, method=1, fps=20):
    processed_videos = []
    for video, rescale in zip(video_list, rescale_params):
        if video is None:
            continue
        video = video.squeeze(0)  # (T, C, H, W)
        if rescale:
            video = (video + 1.0) / 2.0  # [-1,1] -> [0,1]
        video = video.clamp(0, 1)
        processed_videos.append(video)

    if method == 1:
        cat_video = torch.cat(processed_videos, dim=2)
    elif method == 2:
        cat_video = torch.cat(processed_videos, dim=3)
    cat_video = (cat_video * 255).byte().permute(0, 2, 3, 1).cpu().numpy()

    writer = imageio.get_writer(save_path, fps=fps)
    for frame in cat_video:
        writer.append_data(frame)
    writer.close()


def prepare_motion_embeddings(
    num_frames, num_joints, joints_mean, joints_std, theta=10000, device="cuda"
):
    time_embed = get_1d_rotary_pos_embed(44, num_frames, theta, use_real=True)
    time_embed_cos = (
        time_embed[0][:, None, :]
        .expand(-1, num_joints, -1)
        .reshape(num_frames * num_joints, -1)
    )
    time_embed_sin = (
        time_embed[1][:, None, :]
        .expand(-1, num_joints, -1)
        .reshape(num_frames * num_joints, -1)
    )
    spatial_motion_embed = get_3d_motion_spatial_embed(
        84, num_joints, joints_mean, joints_std, theta
    )
    spatial_embed_cos = (
        spatial_motion_embed[0][None, :, :]
        .expand(num_frames, -1, -1)
        .reshape(num_frames * num_joints, -1)
    )
    spatial_embed_sin = (
        spatial_motion_embed[1][None, :, :]
        .expand(num_frames, -1, -1)
        .reshape(num_frames * num_joints, -1)
    )
    motion_embed_cos = torch.cat([time_embed_cos, spatial_embed_cos], dim=-1).to(
        device=device
    )
    motion_embed_sin = torch.cat([time_embed_sin, spatial_embed_sin], dim=-1).to(
        device=device
    )
    return motion_embed_cos, motion_embed_sin


def get_nearest_resize_shape(h, w):
    aspect_ratio = h / w
    ratios = np.array([float(k) for k in ASPECT_RATIO_512.keys()])
    closest_key = str(ratios[np.argmin(np.abs(ratios - aspect_ratio))])
    return ASPECT_RATIO_512[closest_key]


def inference(
    device,
    motion_data_path,
    ref_image_path="",
    output_dir="inference_output",
    prompt="The character is dancing.",
):
    video_length = 49  # Number of frames per clip during training
    overlap = 9  # Number of overlapping frames between adjacent clips during long video inference
    infer_num = 2  # Number of clips to infer for long video generation
    guidance_scale = 6.0  # Classifier-free guidance scale for the text prompt
    fps = 20  # Frame rate of the saved video
    stride = 1  # Temporal stride for frame sampling
    use_first_clip = False  # Whether to use the first frame of the first clip to compute CLIP embeddings during long video inference
    min_num_frames = (video_length - overlap) * (infer_num - 1) + video_length
    weight_dtype = torch.bfloat16
    seed = 42
    generator = torch.Generator(device=device).manual_seed(seed)
    os.makedirs(output_dir, exist_ok=True)
    video_transforms = transforms.Normalize(
        mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True
    )

    # Load data
    with open(motion_data_path, "rb") as f:
        data_list = pickle.load(f)
    if not isinstance(data_list, list):
        data_list = [data_list]

    # Load dataset statistics
    global_mean = np.load("data/mean.npy")
    global_std = np.load("data/std.npy")
    selected_indexes = list(range(0, len(data_list)))

    # download the full repo (cached after the first download)
    base_dir = snapshot_download("yanboding/MTVCrafter")

    lora_path = "data/pretrained_weights/wan2.1/Wan2.1_I2V_14B_FusionX_LoRA.safetensors"

    # Load config
    config = OmegaConf.load(
        Path(os.environ["PROJECT_ROOT"]) / "mtvcrafter/wan2.1/wan_civitai.yaml"
    )

    # Get transformer
    transformer3d = Transformer17B.from_pretrained(
        os.path.join(base_dir, "MV-DiT/Wan-2-1")
    ).to(device, weight_dtype)

    # Get text encoder
    lora_weight_dir = "data/pretrained_weights/wan2.1"
    text_encoder = WanT5EncoderModel.from_pretrained(
        os.path.join(
            lora_weight_dir,
            config["text_encoder_kwargs"].get("text_encoder_subpath", "text_encoder"),
        ),
        additional_kwargs=OmegaConf.to_container(config["text_encoder_kwargs"]),
        torch_dtype=weight_dtype,
    ).to(device, weight_dtype)

    # Get clip image encoder
    clip_image_encoder = CLIPModel.from_pretrained(
        os.path.join(
            lora_weight_dir,
            config["image_encoder_kwargs"].get(
                "image_encoder_subpath", "image_encoder"
            ),
        ),
    )

    # Get vae
    vae = AutoencoderKLWan.from_pretrained(
        os.path.join(lora_weight_dir, config["vae_kwargs"].get("vae_subpath", "vae")),
        additional_kwargs=OmegaConf.to_container(config["vae_kwargs"]),
    ).to(device, weight_dtype)

    # Get scheduler
    scheduler = FlowMatchEulerDiscreteScheduler(
        **filter_kwargs(
            FlowMatchEulerDiscreteScheduler,
            OmegaConf.to_container(config["scheduler_kwargs"]),
        )
    )

    # Get tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        os.path.join(
            lora_weight_dir,
            config["text_encoder_kwargs"].get("tokenizer_subpath", "tokenizer"),
        ),
    )

    # Get motion tokenizer
    motion_encoder = Encoder(
        in_channels=3,
        mid_channels=[128, 512],
        out_channels=3072,
        downsample_time=[2, 2],
        downsample_joint=[1, 1],
    )
    motion_quant = VectorQuantizer(nb_code=8192, code_dim=3072, is_train=False)
    motion_decoder = Decoder(
        in_channels=3072,
        mid_channels=[512, 128],
        out_channels=3,
        upsample_rate=2.0,
        frame_upsample_rate=[2.0, 2.0],
        joint_upsample_rate=[1.0, 1.0],
    )
    ckpt_path = os.path.join(base_dir, "4DMoT/mp_rank_00_model_states.pt")
    state_dict = torch.load(ckpt_path, map_location="cpu")
    vqvae = SMPL_VQVAE(motion_encoder, motion_decoder, motion_quant).to(device)
    print(vqvae.load_state_dict(state_dict["module"], strict=True))

    # Load pipeline
    pipeline = MTVCrafterPipeline17B(
        vae=vae,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        transformer=transformer3d,
        scheduler=scheduler,
        clip_image_encoder=clip_image_encoder,
    )
    pipeline = pipeline.to(device)

    # Add lora to enhance details (recommend)
    if lora_path != "":
        lora_wrapper = WanLoraWrapper(pipeline.transformer)
        for lora_path, lora_scale in zip([lora_path], [1]):
            lora_name = lora_wrapper.load_lora(lora_path)
            lora_wrapper.apply_lora(lora_name, lora_scale)

    # Start inference
    for index in selected_indexes:
        if index == 0 and ref_image_path != "":
            output_dir = (
                f"{output_dir}/{ref_image_path.split('/')[-1].replace('.png', '')}"
            )
            os.makedirs(output_dir, exist_ok=True)

        # read video
        data_dict = data_list[index]
        if data_dict["video_length"] < min_num_frames:
            continue
        input_video_path = data_dict["video_path"]
        vr = VideoReader(input_video_path)

        # save_path
        if ref_image_path == "":
            save_path = os.path.join(
                output_dir,
                f"{index}_{guidance_scale}_{os.path.basename(input_video_path)}",
            )
        else:
            ref_name = os.path.splitext(os.path.basename(ref_image_path))[0]
            save_path = os.path.join(
                output_dir,
                f"{index}_{guidance_scale}_{ref_name}_{os.path.basename(input_video_path)}",
            )

        with torch.no_grad():
            with torch.autocast("cuda", dtype=weight_dtype):
                start_idx = 0
                previous_sample = None
                total_sample = None
                total_pixel_values = None
                total_recon_control_pixel_values = None
                total_ref_images = None
                for clip_index in range(infer_num):
                    indices = np.linspace(
                        start_idx,
                        start_idx + stride * (video_length - 1),
                        video_length,
                        dtype=int,
                    )
                    frames = vr.get_batch(indices).asnumpy()
                    if ref_image_path == "":
                        h, w = frames[0].shape[:2]
                        new_h, new_w = get_nearest_resize_shape(h, w)
                        new_h, new_w = int(new_h), int(new_w)
                    else:
                        ref_image = Image.open(ref_image_path).convert("RGB")
                        ref_w, ref_h = ref_image.size  # PIL uses (width, height)
                        new_h, new_w = get_nearest_resize_shape(ref_h, ref_w)
                        new_h, new_w = int(new_h), int(new_w)
                    frames_resized = np.stack(
                        [cv2.resize(frame, (new_w, new_h)) for frame in frames]
                    )
                    pixel_values = (
                        torch.from_numpy(frames_resized)
                        .permute(0, 3, 1, 2)
                        .contiguous()
                        .unsqueeze(0)
                        / 255.0
                    )
                    pixel_values = video_transforms(pixel_values)

                    # ref pixel value and mask pixel value
                    mask = torch.zeros(
                        (1, video_length, 1, new_h, new_w), dtype=torch.uint8
                    )
                    mask[:, 1:, :, :, :] = 1
                    if ref_image_path == "":
                        ref_image = pixel_values[:, 0:1, :, :, :]
                        mask_pixel_values = pixel_values * (1 - mask)
                    else:
                        ref_image = Image.open(ref_image_path).convert("RGB")
                        ref_image = ref_image.resize((new_w, new_h), Image.BICUBIC)
                        ref_image = (
                            torch.from_numpy(np.array(ref_image))
                            .permute(2, 0, 1)
                            .unsqueeze(0)
                            .float()
                            / 255.0
                        )
                        ref_image = ref_image.unsqueeze(1)
                        ref_image = video_transforms(ref_image)
                        mask_pixel_values = ref_image.repeat(
                            1, video_length, 1, 1, 1
                        ) * (1 - mask)
                    clip_pixel_values = (
                        ref_image[0][0].permute(1, 2, 0).contiguous() * 0.5 + 0.5
                    ) * 255
                    clip_image = Image.fromarray(np.uint8(clip_pixel_values))

                    # smpl poses
                    try:
                        smpl_poses = np.array(
                            [
                                pose[0][0].cpu().numpy()
                                for pose in data_dict["pose"]["joints3d_nonparam"][
                                    :min_num_frames
                                ]
                            ]
                        )
                        poses = smpl_poses[indices]
                    except:
                        poses = data_dict["pose"][indices]

                    norm_poses = torch.tensor(
                        (poses - global_mean) / global_std
                    ).unsqueeze(0)
                    motion_tokens, vq_loss = vqvae(
                        norm_poses.to(device), return_vq=True
                    )
                    print(f"vq loss: {vq_loss}")
                    motion_rotary_emb = prepare_motion_embeddings(
                        (video_length - 1) // 4 + 1,
                        24,
                        global_mean,
                        global_std,
                        device=device,
                    )
                    recon_motion = (
                        vqvae(norm_poses.to(device))[0][0]
                        .to(dtype=torch.float32)
                        .cpu()
                        .detach()
                        * global_std
                        + global_mean
                    )
                    recon_control_pixel_values = get_control_conditions(
                        recon_motion, new_h, new_w
                    )

                    print(f"{index} --- {clip_index} --- {indices[0]} --- {prompt}")
                    sample = pipeline(
                        prompt=prompt,
                        num_frames=video_length,
                        num_inference_steps=10 if lora_path != "" else 50,
                        negative_prompt="bad hands, extra limbs, fused fingers, blurry, low quality",
                        height=new_h,
                        width=new_w,
                        guidance_scale=guidance_scale,
                        generator=generator,
                        video=pixel_values,
                        mask=mask,
                        mask_video=mask_pixel_values,
                        clip_image=first_clip_image
                        if use_first_clip and index != 0
                        else clip_image,
                        ref_image=ref_image,
                        control_video=None,
                        motion_tokens=motion_tokens,
                        motion_rotary_emb=motion_rotary_emb,
                        previous_sample=previous_sample,
                        lora_path=lora_path,
                        use_first_clip_image=use_first_clip,
                        overlap=overlap,
                    ).videos

                    if clip_index == 0:
                        if use_first_clip:
                            first_clip_image = clip_image
                        total_sample = sample[:, :, :-overlap]
                        total_pixel_values = pixel_values[:, :-overlap]
                        total_recon_control_pixel_values = recon_control_pixel_values[
                            :, :-overlap
                        ]
                        if infer_num == 1:
                            total_sample = sample
                            total_pixel_values = pixel_values
                            total_recon_control_pixel_values = (
                                recon_control_pixel_values
                            )
                            total_ref_images = ref_image.repeat(
                                1, video_length, 1, 1, 1
                            )
                        else:
                            total_ref_images = ref_image.repeat(
                                1, video_length - overlap, 1, 1, 1
                            )
                    elif clip_index != infer_num - 1:
                        total_sample = torch.cat(
                            [total_sample, sample[:, :, :-overlap]], dim=2
                        )
                        total_pixel_values = torch.cat(
                            [total_pixel_values, pixel_values[:, :-overlap]], dim=1
                        )
                        total_recon_control_pixel_values = torch.cat(
                            [
                                total_recon_control_pixel_values,
                                recon_control_pixel_values[:, :-overlap],
                            ],
                            dim=1,
                        )
                        total_ref_images = torch.cat(
                            [
                                total_ref_images,
                                previous_sample[:, :, -1:]
                                .transpose(1, 2)
                                .repeat(1, video_length - overlap, 1, 1, 1)
                                * 2
                                - 1,
                            ],
                            dim=1,
                        )
                    else:
                        total_sample = torch.cat([total_sample, sample], dim=2)
                        total_pixel_values = torch.cat(
                            [total_pixel_values, pixel_values], dim=1
                        )
                        total_recon_control_pixel_values = torch.cat(
                            [
                                total_recon_control_pixel_values,
                                recon_control_pixel_values,
                            ],
                            dim=1,
                        )
                        total_ref_images = torch.cat(
                            [
                                total_ref_images,
                                previous_sample[:, :, -1:]
                                .transpose(1, 2)
                                .repeat(1, video_length, 1, 1, 1)
                                * 2
                                - 1,
                            ],
                            dim=1,
                        )

                    previous_sample = sample[:, :, -overlap:]
                    start_idx = indices[-overlap]

                print(f"save to {save_path}")
                concat_and_save(
                    [
                        total_pixel_values,
                        total_ref_images,
                        total_recon_control_pixel_values,
                        total_sample.transpose(1, 2),
                    ],
                    save_path,
                    rescale_params=[True, True, True, False],
                    method=2,
                    fps=fps,
                )


def main(
    ref_image_path: str = "",
    motion_data_path: str = "data/sampled_data.pkl",
    output_dir: str = "inference_output",
    prompt: str = "The character is dancing.",
):
    """MTVCrafter 17B inference script.

    Args:
        ref_image_path: Path to the reference character image
        motion_data_path: Path to the motion sequence
        output_dir: Where to save the generated video
        prompt: Text prompt of the generated video
    """
    inference(
        device="cuda:0",
        motion_data_path=motion_data_path,
        ref_image_path=ref_image_path,
        output_dir=output_dir,
        prompt=prompt,
    )


if __name__ == "__main__":
    tyro.cli(main)
